\section{Literature Review}
\subsection{Problems of Software Engineering}
Software has become an integral part of our daily lives. Certain expectations exist regarding the quality of software in terms of reliability, security and efficiency. These expectations come with challenges for the software developers across all industries.
For decades, software engineers have tried to develop methods and guides to overcome these issues. However, in 1986 Frederick Brooks published his paper `No Silver Bullet'
in which he argues that: `\ldots building software will always be hard. There is inherently no silver bullet.' \footcite[3]{brooksNoSilverBullet1987}. He based this statement on the fact that there two types of difficulties in software development: the essential and the accidental.
The essential difficulties he names are complexity, conformity, changeability and invisibility.\\
With complexity, Brooks wants to describe the inherit intricacy of software systems: `Software entities are more complex for their size than perhaps any other human construct, because no two parts are alike'.\footcite[3]{brooksNoSilverBullet1987}
This complexity makes `conceiving, describing, and testing them hard'\footcite[3]{brooksNoSilverBullet1987}.\\
The second essential difficulty Brooks names is conformity. To explain this, he compares software development to physics. Even though they are similarly complex, physics has the advantage of relying on a single set of laws or `creator'. The same cannot be said for software engineers. Brooks claims that
the complexity is `arbitrary [\ldots], forced without rhyme or reason by the many human institutions and systems to which his interfaces must conform'\footcite[4]{brooksNoSilverBullet1987}. This is due to software being perceived as `the most comfortable'\footcite[4]{brooksNoSilverBullet1987} element to change in a system.\\
Brooks explains the third issue, changeability, by comparing software to other products like cars or computers. With these types of products, changes are difficult to make once the product is released. Software however is just `pure thought-stuff, infinitely malleable.'\footcite[4]{brooksNoSilverBullet1987} Another major issue regarding changeability is
the fact that software often `survives beyond the normal life of the machine vehicle for which it is first written'\footcite[4]{brooksNoSilverBullet1987}. This means that software has to be adapted to new machines causing an extended life time of the software.\\
Invisibility is the last essential difficulty Brooks names. With this he means the difficulty to visualize software compared to other products. This not only makes the creation difficult but also `severely hinders communication among minds'\footcite[4]{brooksNoSilverBullet1987}.
According to Brooks, these issues are in `the very nature of software' \footcite[2]{brooksNoSilverBullet1987}. These difficulties are unlikely to be solved, unlike the accidental difficulties.\\

In contrast, the accidental difficulties arise from limitations of current languages, tools and methodologies. According to Brooks, this involves issues such as inefficient programming environments, suboptimal development processes and integration challenges which can be overcome as the industry improves its practices and technologies.\footcite[5-6]{brooksNoSilverBullet1987}
For example, the adaptation of agile methodologies, integrated development environments and continuous integration have helped to overcome some of these accidental difficulties.\\

The persistent nature of these challenges presented by Brooks have since been substantiated by further empirical research. For instance, Lehman and Ramil (2003) discussed in their paper `Software evolution—Background, theory, practice' that software systems which are left unchecked will experience a decline in quality over time.\footcite[34]{lehmanSoftwareEvolutionBackground2003}
This phenomenon is encapsulated in Lehman's laws of software evolution, which he formulated in multiple papers.  Lehman and Ramil present empirical observations supporting the notion that software quality tends to deteriorate over time\footcite[42]{lehmanSoftwareEvolutionBackground2003} - a phenomenon often described as
software decay.

\subsection{Software Decay and Technical Debt}
The term software decay or erosion was empirically studied and statistically validated by Eick et al.\ in their influential paper `Does Code Decay? Assessing the Evidence from Change Management Data'(2001). They begin by stating that `software does not age or "wear out" in the conventional sense.' \footcite[1]{eickDoesCodeDecay2001}
If nothing in the environment changes, the software could run forever. However, this is almost never the case as there is a constant change in several areas. This is predominantly with respect to the two areas of the hard- and software environments and the requirements of the software.\\

This is in accordance with the first two laws of Program Evolution Dynamics formulated by Belady and Lehman (1976).
The first law states: `A system that is used undergoes continuing change until it is judged more cost effective to freeze and recreate it.'\footcite[228]{beladyModelLargeProgram1976}
Building on this, their second law declares: `The entropy of a system (its unstructuredness) increases with time, unless specific work is executed to maintain or reduce it.'\footcite[228]{beladyModelLargeProgram1976}\\

The analysis of Eick et al.\ provide empirical validation for these theoretical laws, offering `very strong evidence that code does decay.'\footcite[7]{eickDoesCodeDecay2001}
This conclusion is based on their findings that `the span of changes increases over time'\footcite[7]{eickDoesCodeDecay2001} meaning that modifications to the software tend to affect increasingly larger parts of the system as the software evolves. This growth in the span of changes indicates and potentially leads to
a breakdown in the software's modularity. Consequently the software becomes `more difficult to change than it should be,'\footcite[3]{eickDoesCodeDecay2001} measured specifically by three criteria: cost of change, time to implement change and the resulting quality of the software.\footcite[3]{eickDoesCodeDecay2001}
Therefore, the combination of theoretical insights from Lehman and Belday and empirical data from Eick et al. paints a clear picture: software decay is an inevitable consequence of ongoing evolution unless consciously and proactively managed through structured efforts such as continuous refactoring and architectural vigilance.\\

The concept of software decay aligns closely with earlier theoretical discussions by David Parnas (1994). In his influential paper `Software Aging', Parnas describes software aging as a progressive deterioration of a program's internal quality primarily due to frequent, 
inadequately documented modifications which he termed `Ignorant surgery'\footcite[280]{296790}, as well as the failure to continuously adapt the architecture to evolving needs which he called `Lack of movement'\footcite[280]{296790}.
Without this proactive maintenance and refactoring effort, Parnas argues that software inevitably reaches a state where changes become more risky, costly and error-prone\footcite[280-281]{296790}.

//TODO: Emperical studies like Banker or Bieman maybe?

The term `Technical Debt' was first coined by Ward Cunningham in his paper `The WyCash Portfolio Management System' (1992). This metaphor was used to describe the trade-off between a quickly implemented solution and a thought-out process. 
Use of using the quick solution `is like going into debt.'\footcite[2]{cunninghamWyCashPortfolioManagement1992} Cunningham argues that this debt accumulates interest if not repaid or rewritten. 
If this does not happen, Cunningham warns that `Entire engineering organizations can be brought to a stand-still under the debt load of an unconsolidated implementation'\footcite[2]{cunninghamWyCashPortfolioManagement1992}.\\

This term was further built upon and refined by the industry through white papers like `Technical Debt' by Steve McConnell (2008) or the `Technical Debt Quadrant' by Martin Fowler (2009).
McConnell differentiates between two types of technical debt: Unintentional and Intentional \footcite[3]{mcconnellManagingTechnicalDebt2017}. The first results from bad code, inexperience or unknowingly taking over a project with technical debt.
The second type is taken on purpose `to optimize for the present rather than for the future.'\footcite[3]{mcconnellManagingTechnicalDebt2017} As the first is not planned, it is difficult to avoid. The second type, however, can be managed and controlled.\\
Additionally, McConnell differentiates between different types of intentional debt. According to him, debt can be taken on a short-term or long-term basis. The short-term debt is taken on to meet a deadline or to deliver a feature. Therefore it is `taken on tactically or reactively'\footcite[3]{mcconnellManagingTechnicalDebt2017}.
The long-term debt on the other hand is more strategic and is taken on to help the team in a larger context. The difference between those two is that short-term debt `should be paid off quickly, perhaps as the first part of the next release cycle'\footcite[4]{mcconnellManagingTechnicalDebt2017}, while 
long-term debt can be carried by companies for years.\\
Martin Fowler, on the other hand, warned against taking on too much deliberate debt. He argues that `Even the best teams will have debt to deal with as a project goes on - even more reason not to overload it with crummy code.'\footcite{fowlerTechnicalDebtQuadrant2009}
He created a quadrant between reckless and prudent and deliberate and inadvertent debt. For Fowler, the difference between reckless and prudent is the way the debt is taken on. Reckless debt happens without an appropriate evaluation of the consequences, risking difficulties in the future. Alternatively prudent debt is taken on
with the trade-offs in mind and the knowledge of the future costs. Fowler differentiates between deliberate and inadvertent in a similar way to McConnell's differentiation between intentional and unintentional debt.
The various combinations of these four elements in the quadrant results in four different approaches. Reckless and deliberate would mean quick solutions without considering the long-term impact. Reckless and inadvertent results in flawed design or implementation, either carelessly or unknowingly. 
Prudent and deliberate is purposefully taking on debt to gain a short-term advantage with plans of repayment and finally prudent and inadvertent means taking on debt due to lack of knowledge or experience.\footcite{fowlerTechnicalDebtQuadrant2009}\\

In their article `Technical Debt: From Metaphor to Theory and Practice' (2012) Kruchten et al.\ criticize the concept of technical to be `somewhat diluted lately' \footcite[18]{kruchtenTechnicalDebtMetaphor2012}, stating that every issue in software development was called some form of debt. 
Therefore they set out to define `a theoretical foundation'\footcite[19]{kruchtenTechnicalDebtMetaphor2012} for technical debt.\\
Kruchten et al.\ state that technical debt has become more than the initial coding shortcuts and rather encompasses all kinds of internal software quality comprises.\footcite[19]{kruchtenTechnicalDebtMetaphor2012}
According to them, this includes architectural debt, `documentation and testing'\footcite[20]{kruchtenTechnicalDebtMetaphor2012} as well as requirements and infrastructure debt.
All these debt types allow engineers to better discuss the trade-offs with stakeholders and to make better decisions.\\
\\TODO: More about Kruchten and Theory?

There have been many studies providing empirical evidence for the theoretical concepts of technical debt. Highly influential studies were undertaken by Potdar and Shihab (2014) as well as by Li et al.\ (2015)\\
In their study Potdar and Shihab analyzed four large open source projects to find self-admitted technical debt as well as the likelihood of debt being removed. They found that `self-admitted technical debt exists in 2.4\% to 31\% of the files.'\footcite[1]{potdarExploratoryStudySelfAdmitted2014}
Additionally, they found that `developers with higher experience tend to introduce most of the self-admitted technical debt and that time pressures and complexity of the code do not correlate with the amount of the self-admitted technical debt.'\footcite[1]{potdarExploratoryStudySelfAdmitted2014}
They also discovered that `only between 26.3\% and 63.5\% of the self-admitted technical debt gets removed'\footcite[1]{potdarExploratoryStudySelfAdmitted2014}. This relatively low removal rate of self-admitted technical debt indicates a wider challenge:
developers recognize the issues of their implementation, but defer remediation potentially leading to a major impact on long-term maintainability.\\
Another approach to provide empirical evidence towards technical debt was taken by Li et al.\ . They conducted a systematic mapping study to `get a comprehensive understanding of the concept of "technical debt"'\footcite[194]{liSystematicMappingStudy2015}, as well as getting an overview of the current research in the field.
Areas of investigation included existing types of technical debt (TD), the effect of technical debt on software quality and quality attributes (QAs) as well as the limit of the technical debt metaphor.\\
They established that the `10 types of TD are requirements TD, architectural TD, design TD, code TD, test TD, build TD, documentation TD, infrastructure TD, versioning TD, and defect TD.'\footcite[215]{liSystematicMappingStudy2015}
Additionally they found that `[m]ost studies argue that TD negatively affects the maintainability [\ldots] while other QAs and sub-QAs are only mentioned in a handful of studies'\footcite[215]{liSystematicMappingStudy2015}.\\
During their studies,  Li et al. observed that the inconsistent and arbitrary use of the term `debt' among researchers and practitioners can cause confusion and hinder effective management of technical debt.\footcite[211]{liSystematicMappingStudy2015} Additionally practitioners `tend to connect any software quality issue to debt, such as code smells debt, dependency debt and usability debt.'\footcite[212]{liSystematicMappingStudy2015}
This indicates an inflationary use of the term, which is important to keep in mind when speaking about technical debt.\\

The implications these studies have for the software industry are significant. They show that software decay and technical debt are tangible and measurable in real world software projects.
In their paper `Software complexity and maintenance costs' (1993) Banker et al. empirically demonstrated that `software maintenance costs are significantly affected by the levels of existing software complexity.' \footcite[12]{bankerSoftwareComplexityMaintenance1993}
This finding emphasizes the important of proactively managing the software quality and addressing debt early in the project lifecycle, to keep the complexity and therefore cost to a minimum.\\
To address these effects, practitioners strongly recommend refactoring. Fowler argued in his book `Refactoring: Improving the Design of Existing Code' (2019)
that `[w]ithout refactoring, the internal design - the architecture - of software tend to decay.'\footcite[58]{fowlerRefactoringImprovingDesign2019}
To prevent this, he suggests `[r]egular refactoring [to] help[s] keep the code in shape'\footcite[58]{fowlerRefactoringImprovingDesign2019}.\\
To prevent long-term issues, practitioners recommend to actively manage technical debt through refactoring, tracking and other strategies 
that integrate debt management into the software development process.\\

In summary, software decay describes the gradual deterioration of software quality over time, driven continuous modifications, environmental changes and increasing complexity. This as theoretically established Belady, Lehman and Parnas and empirically validated by Eick et al. and others.
Closely related yet conceptually distinct, technical debt captures the intentional and unintentional compromises made during software development. This leads to future costs if not proactively managed.
While decay views a border spectrum of deterioration, technical debt specifically highlights how certain software engineering decisions increases the problem. 
Empirical research by Potdar and Shihab, Li et al., and Banker et al. underscores the tangible impact technical debt and software complexity have on real-world maintenance costs and software quality.
Consequently, addressing software decay effectively in practice demands a combination of proactive quality management, regular refactoring, explicit technical debt tracking and structured maintenance strategies.
With these foundational concepts clarified, the next section explores concrete mitigation strategies employed in commercial software environments, providing valuable insights into preventing software decay within the unique constraints of military software development.



\subsection{Mitigation Strategies in Commercial Environments}
Technical debt and software decay have been recognized as significant challenges in the software industry. They can lead to 
increased maintenance costs, reduced software quality and decreased developer productivity, overall leading to a more expensive and less competitive product.
To address the challenges, practitioners and researchers have developed a variety of strategies to manage, prevent and mitigate technical debt.
This section will provide an overview of the most common strategies, techniques and frameworks used in commercial environments to address technical debt.\\

\subsubsection{High-Level Mitigation Strategies}
To efficiently mitigate software decay and technical debt, proactive management strategies are essential. These strategies aim to prevent the accumulation of 
debt and address software entropy and decay directly by integrating quality assurance directly into everyday development process.
Such strategies include Agile methodologies like Scrum or \ac{XP} as well as technical practices
such as \ac{CI/CD}. These practices are designed to embed ongoing maintenance and quality assurance into routine workflows, thus combating software entropy at its core.\\
Agile methods emphasize frequent iterations, close collaboration between developers and stakeholders as well as continuos refactoring to prevent the
gradual degradation of the software quality and mitigation software decay.\\
Similarly, \ac{CI/CD} introduces rigorous automation, rapid feedback loops and early detection of defects to proactively controlling both technical debt accumulation
and broader software quality decay. Collectively, these methodologies create a culture of continuous improvement, adaptability and quality assurance, 
ensuring software maintainability and long-term project sustainability.\\
\paragraph{Agile Methodologies}
In their paper `Technical Debt Management in Agile Software Development: A Systematic Mapping Study' (2024)\footcite{leiteTechnicalDebtManagement2024} Leite et al.
investigated how agile methods can be used to manage technical debt. They found that `\ldots Scrum and Extreme Programming are the most utilized methodologies 
for managing technical debt.'\footcite[318]{leiteTechnicalDebtManagement2024} While this study focuses explicitly on technical debt, both Scrum and \ac{XP}
also inherently address the boarder issue of software decay by encouraging proactive quality management and continuous improvement practices.\\
Scrum was first presented by Ken Schwaber in his paper `SCRUM Development Process' (1995)\footcite{schwaberSCRUMDevelopmentProcess1997}. 
It has since become on of the most popular agile frameworks in the software industry. Scrum explicitly manages software quality and technical debt through iterative cycles called sprints.
After each sprint, the team reflects on their work, identifies quality issues, technical debt and potential decay indicators and plans improvements in
retrospectives. Leite et al. found that for identifying technical debt in Scrum the Sprint and Product Backlog are the most used artifacts.\footcite[315]{leiteTechnicalDebtManagement2024}
By explicitly managing these items with their workflow, teams effectively reduce both debt and software entropy, improving overall software maintainability.\\

\ac{XP}, first introduced by Kent Beck in his influential book `Extreme Programming Explained' (1999)\footcite{beckExtremeProgrammingExplained1999},
explictly integrates practices to enhance software quality and prevent software decay directly.
\ac{XP} practices such as pair programming, \ac{TDD} and \ac{CI} and continuos refactoring help maintain high software quality, thus preventing both debt accumulation
and broader software entropy.
Pair programming prevents decay by ensuring higher-quality code through collaborative review and knowledge sharing between developers.
Continuous Integration ensures regular, frequent code integration, significantly reducing integration complexity and associated decay risks.
\ac{TDD} ensures robust test coverage, catching defects early and preventing quality erosion.
Refactoring, a cornerstone XP practice, has proven its effectiveness empirically; for example, Moser et al.
demonstrated in their case study(2008)\footcite{moserCaseStudyImpact2008} that refactoring explicitly `prevents an explosion of complexity'\footcite[262]{moserCaseStudyImpact2008}
and promotes simpler, easier-to-maintain designs.
They found it drove developers toward simpler designs, reducing complexity, coupling, and long-term maintenance issues—directly counteracting software decay.\footcite[262]{moserCaseStudyImpact2008}
Beck argues that \ac{XP}'s incremental, continuos quality practices consistently maintain software quality and adaptability throughout development, directly addressing both debt and broader software decay.\\
Overall Agile methodologies, particularly Scrum and \ac{XP}, systematically manage technical debt and proactively prevent software decay by fostering continuous improvement, structured quality management and adaptability.
Complementing these Agile practices, the adoption of automated \ac{CI/CD} pipelines further enhances the proactive management of both technical debt
and broader software decay through rigorous quality control and systematic automation.\\

\paragraph{Continuous Integration/Continuous Deployment}
\ac{CI} was first introduced by Beck in the context of \ac{XP} and later refined by Martin Fowler in his influential article `Continuous Integration'\footcite{fowlerContinuousIntegration2006}.
Fowler describes \ac{CI} as not only the frequent, automated integration of code into the main repository but also the systematic automation of building and testing process.
According to Fowler, `Self-testing code is so important to Continuous Integration that it is a necessary prerequisite.'\footcite{fowlerContinuousIntegration2006}.
Furthermore, another critical prerequisite is `that they can correctly build the code,'\footcite{fowlerContinuousIntegration2006} thus guaranteeing that code changes consistently
integrate without issues.\\

To further prevent technical debt and broader software decay, a quality analysis tools (e.g. static code analyzers such as SonarQube or DeepSource) are frequently integrated into
\ac{CI} pipelines. These tools often provide a metric to evaluate technical debt which is calculated based on the effort in minutes to fix the found maintainability issues.\footcite{sonarqubeUnderstandingMeasuresMetrics2025}
In their paper `Technical Debt Measurement during Software Development using Sonarqube: Literature Review and a Case Study' (2021)\footcite{murilloTechnicalDebtMeasurement2021}
Murillo et al. found that SonarQube is a useful tool for early debt detection. The estimated remediation effort metric allows for a good debt management prioritization.\footcite[5]{murilloTechnicalDebtMeasurement2021}
However during their research they noticed if they changed SonarQubes default rules by just 26 rules, the technical debt effort would increase from
1 hours and 50 minutes to 11 hours.\footcite[4]{murilloTechnicalDebtMeasurement2021} This and the fact that SonarQube can only detect code related debt and not for example
infrastructure or requirements debt, makes these tools useful but not a complete solution.\\

\ac{CD}, introduced by Jez Humble and David Farley in their foundational book `Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation' (2010)\footcite{humbleContinuousDeliveryReliable2010}
extends \ac{CI} by automating the entire software release pipeline. \ac{CD} ensures that the software is always in a releasable state.
According to Humble and Farley implementing a functional \ac{CD} pipeline `creates a release process that is repeatable, reliable, and predictable'\footcite[17]{humbleContinuousDeliveryReliable2010}.
Beyond predictability, additional significant benefits include team empowerment, deployment flexibility and substantial error reduction.
Specially, \ac{CD} effectively reduces errors, particularly those introduced by poor configuration management, including problematic areas such as
`configuration files, scripts to create databases and their schemas, build scripts, test harnesses, even development environments and operating system configurations'\footcite[19]{humbleContinuousDeliveryReliable2010}.\\

Empirical evidence from academic studies clearly demonstrates the effectiveness of \ac{CI/CD} in reducing both technical debt and software decay.
For instance, 
TODO: Find emperical evidence for CI/CD\\
\subsubsection{Code-Level Practices for Preventing Decay}
On the code level, a variety of practices can be used to prevent software decay and technical debt. The two major have been previously introduced: continuous refactoring and maintaining software quality.\\
The benefits of refactoring have been demonstrated int the previous section. However, deeper empirical studies illustrate the impacts and challenges in a commercial environment.
Kim et al. (2014) observed in their study `An empirical study of refactoring challenges and benefits at Microsoft' observed that `[d]evelopers perceive that refactoring involves substantial cost and risks'\footcite[17]{kimEmpiricalStudyRefactoringChallenges2014}.
Additionally they found that the benefits refactoring brings `multidimensional'\footcite[17]{kimEmpiricalStudyRefactoringChallenges2014} and that its not consistent across different metrics, which is why they recommended a tool to monitor the impact of refactoring across these metrics.
Similar, Tempero et al. (2017) found certain barriers to refactoring in their study `Barriers to refactoring'. They found that at least 40\% of the developers in their study would not refactor classes, even though they thought it would be beneficial. \footcite[60]{temperoBarriersRefactoring2017}
Tempero et al. claims the reasons were `lack of resources, of information identifying consequences, of certainty regarding risk, and of support from management.'\footcite[60]{temperoBarriersRefactoring2017} even though developers did not state a lack of refactoring tools as a reason.
Tempore et al. suggested to eliminate these barriers, refactoring should be goal-oriented instead of operations-oriented and a better quantification of the benefits refactoring should bring to better reach a decision.\footcite[61]{temperoBarriersRefactoring2017}\\
Both studies show that the theoretical benefits of refactoring are not always directly translated into the industry. Developers often perceive refactoring as risky and costly, even though they see the benefits.\\
\TODO: Write more about this

\subsubsection{Architectural Strategies for Long-Term Qualtiy}
In addition to code-level practices, maintaining the software architecture is crucial to prevent software decay and technical debt. The architecture was created as a solution to a problem and it should be ensured that the architecture erosion is prevented.
This erosion happens through changing requirements. In an attempted to meet them, developers often make changes to the architecture.\footcite[1]{desilvaControllingSoftwareArchitecture2012} These changes can lead to a degradation of the architecture and the introduction of technical debt.
To prevent this, a variety of strategies can be employed. These have been investigated by De Silvia and Balasubramaniam (2012).\\
One option they named is process-oriented architecture conformance.\footcite[135]{desilvaControllingSoftwareArchitecture2012} This practice is used to ensure that code changes do not violate high level design rules. To enable this process, the required documentation, monitoring and dependency analysis need to be in place.\footcite[135]{desilvaControllingSoftwareArchitecture2012}
This enables teams to detect violations through architecture compliance checking tools, preventing the drift away from the original architecture.
\\TODO: Write more about thsis

\subsubsection{Process and Organizational Measures}
Organizational and process-oriented practices form the third pillar in combating software decay.
One crucial practice is the management of technical debt. Many companies track technical debt items like outdated modules, quick fixes or known architectural shortcomings.
This can be done through issue trackers or backlogs to allow for structured approach and time allocation to address them regularly.
An industry-wide survey conducted by Ramač et al. (2021) found that 47\% `had some practical experience with TD identification and/or management'\footcite[40]{ramacPrevalenceCommonCauses2021}.
By visualizing and tracking technical debt, teams can prioritize and address debt items systematically, preventing their accumulation and broader software decay.
Ramač et al. also found that the most common cause for technical debt was time pressure caused by deadlines\footcite[40]{ramacPrevalenceCommonCauses2021} and the `single most cited effect of TD is delivery delay.'\footcite[40]{ramacPrevalenceCommonCauses2021}
This indicates that time pressure is not only a cause for technical debt but also a consequence, leading to a vicious cycle of debt accumulation and delivery delays. To break this cycle, a balance of new development and maintenance is crucial to prevent a border software decay.
This is promoted by agile methodologies through principle of continuos improvement, through practices like including refactoring in their definition of done or allowing dedicated maintenance sprints.

Another important practice is conducting regular code reviews as part of the development workflow. This practice dates back to 1976 when Michael Fagan reviewed the benefits of peer code inspections.\footcite[183]{faganDesignCodeInspections1976}
He found that `inspections increase productivity and improve final program quality.'\footcite[205]{faganDesignCodeInspections1976}
Since then code reviews have become more lightweight compared to the inspection proposed by Fagan, increasing participation. By removing in-person meetings and reviewer checklists, code reviews have become a standard practice in software development and usually occur before code is merged into the main repository.
McIntosh et al. (2016) investigated the impact of code reviews on software quality, by comparing the review coverage, participation and expertise of the reviewer against the post-release defects.\footcite[6]{mcintoshEmpiricalStudyImpact2016}
They found that coverage, while important, is not the only factor that influences the post-release defects.\footcite[39]{mcintoshEmpiricalStudyImpact2016}
They state that `review participation should be considered when making integration decisions.'\footcite[39]{mcintoshEmpiricalStudyImpact2016}
Additionally, they recommend that if an expert in the matter is not available for the original code, they should be included in the review process to prevent defects.\footcite[39]{mcintoshEmpiricalStudyImpact2016}

In conclusion, having a structured approach to managing technical debt and prevent software decay is crucial to maintaining software quality and long-term project sustainability.
Process-integrated practices like code reviews, explicit technical debt control and a culture of continuous refactoring create an environment that proactively manages software decay and technical debt.

\subsubsection{Tool Support for Continous Quality Assurance}
To further support the proactively manage software decay, technical debt and the overall code health over time, a variety of tools have established themselves in the industry.
Automated quality assurance tools are often integrated into modern development processes. A common approach, as previously mentioned, is the use of \ac{CI} pipelines. These can be used in combination with quality gates that use static code analysis to block additions to the code base that do not meet the quality standards e.g. a 
certain code coverage, complexity or maintainability threshold. This allows developers to catch issues early before they are introduced into the code base and rectify them. While the concept of quality gates is not new, the automation of these gates have been investigated by Uzunova et al. (2024).
They found that these gates can serve as a check point to assess software metrics like code coverage, bug density or compliance with coding standards. \footcite[8]{uzunovaQualityGatesSoftware2024}
To allow for this kind of automation, they mentioned tools like SonarQube, Sigrid or Maverix.ai. These tools allow for real-time feedback allowing for an enforcement of quality criteria.\footcite[8]{uzunovaQualityGatesSoftware2024}\\










